# Ollama needs enough RAM for the chat model (e.g. llama3.2:3b ~2.3 GiB).
# On Colima: colima stop && colima start --memory 4
# Low-memory option: set CHAT_OLLAMA_MODEL=tinyllama and OLLAMA_PULL_MODELS=tinyllama mxbai-embed-large
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    deploy:
      resources:
        limits:
          memory: 4G
    restart: unless-stopped

  # One-off: pull Ollama models into the ollama service (set OLLAMA_PULL_MODELS in .env to add more)
  ollama-pull:
    image: ollama/ollama:latest
    environment:
      OLLAMA_HOST: http://ollama:11434
      OLLAMA_PULL_MODELS: ${OLLAMA_PULL_MODELS:-llama3.2:3b mxbai-embed-large}
    entrypoint: ["/bin/sh", "-c"]
    command:
      - "sleep 15 && for m in $$OLLAMA_PULL_MODELS; do ollama pull $$m; done"
    depends_on:
      - ollama
    restart: "no"

  qdrant:
    image: qdrant/qdrant:latest
    container_name: rag-qdrant
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage
    restart: unless-stopped

  postgres:
    image: pgvector/pgvector:pg16
    container_name: rag-postgres
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: agentic_rag
    ports:
      - "5432:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data
    restart: unless-stopped

  api:
    build: .
    container_name: rag-api
    ports:
      - "3000:3000"
    environment:
      PORT: "3000"
      DATABASE_URL: "postgresql://user:password@postgres:5432/agentic_rag?schema=public"
      OLLAMA_BASE_URL: "http://ollama:11434"
      CHAT_OLLAMA_MODEL: "${CHAT_OLLAMA_MODEL:-llama3.2:3b}"
      EMBEDDINGS_OLLAMA_MODEL: "${EMBEDDINGS_OLLAMA_MODEL:-mxbai-embed-large}"
      RAG_RETRIEVAL_K: "8"
      QDRANT_URL: "${QDRANT_URL:-http://qdrant:6333}"
      QDRANT_COLLECTION: "${QDRANT_COLLECTION:-rag_docs}"
      FRONTEND_URL: "http://localhost:3001"
      LOG_PRETTY: "${LOG_PRETTY:-1}"
      OPENROUTER_API_KEY: "${OPENROUTER_API_KEY:-}"
      GEMINI_API_KEY: "${GEMINI_API_KEY:-}"
    depends_on:
      - ollama
      - postgres
      - qdrant
    restart: unless-stopped

  web:
    build:
      context: ./web
      dockerfile: Dockerfile
    container_name: rag-web
    ports:
      - "3001:3001"
    environment:
      NEXT_PUBLIC_API_URL: "http://localhost:3000"
      HOSTNAME: "0.0.0.0"
      PORT: "3001"
    depends_on:
      - api
    restart: unless-stopped

volumes:
  ollama_data:
  pg_data:
  qdrant_data: